{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309d8016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6d56e",
   "metadata": {},
   "source": [
    "- 라이브러리 import 및 Huggingface 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"hf_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4385118",
   "metadata": {},
   "source": [
    "- Model/Dataset 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb9814",
   "metadata": {},
   "source": [
    "# Hugging Face Basic Model 한국어 모델\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"\t# beomi님의 Llama3 한국어 파인튜닝 모델\n",
    "\n",
    "# baemin_dataset_simple.json\n",
    "hkcode_dataset = \"/content/dataset\"\n",
    "\n",
    "# 새로운 모델 이름\n",
    "new_model = \"Llama3-Ko-3-8B-baemin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bc88d",
   "metadata": {},
   "source": [
    "- 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hkcode_dataset, split=\"train\")\n",
    "\n",
    "# dataset = dataset.select(range(200))\n",
    "\n",
    "# 데이터 확인\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893894cc",
   "metadata": {},
   "source": [
    "- GPU 환경 및 attention 메커니즘 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 사용중인 GPU의 CUDA 연산 능력을 확인한다.\n",
    "# 8이상이면 고성능 GPU 로 판단한다.\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    # 고성능 Attention인 flash attention 2 을 사용\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    # 데이터 타입을 bfloat16으로 설정해준다.\n",
    "    # bfloat16은 메모리 사용량을 줄이면서도 계산의 정확성을 유지할 수 있는 데이터 타입이다.\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16298c4",
   "metadata": {},
   "source": [
    "- QLoRA를 사용한 4비트 양자화 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\t# 모델 가중치를 4비트로 로드\n",
    "    bnb_4bit_quant_type=\"nf4\",\t# 양자화 유형으로는 “nf4”를 사용한다.\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\t# 양자화를 위한 컴퓨팅 타입은 직전에 정의 했던 torch_dtype으로 지정 해준다.\n",
    "    bnb_4bit_use_double_quant=False,\t# 이중 양자화는 사용하지 않는다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba55d0",
   "metadata": {},
   "source": [
    "- 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eec8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\t# 0번째 gpu 에 할당\n",
    ")\n",
    "# 모델의 캐시 기능을 비활성화 한다. 캐시는 이전 계산 결과를 저장하기 때문에 추론 속도를 높이는 역할을 한다. 그러나 메모리 사용량을 증가시킬 수 있기 때문에, 메모리부족 문제가 발생하지 않도록 하기 위해 비활성화 해주는 것이 좋다.\n",
    "model.config.use_cache = False\n",
    "# 모델의 텐서 병렬화(Tensor Parallelism) 설정을 1로 지정한다. 설정값 1은 단일 GPU에서 실행되도록 설정 해주는 의미이다.\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4f316",
   "metadata": {},
   "source": [
    "- 토크나이저 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92919a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model,\n",
    "              trust_remote_code=True)\n",
    "# 시퀀스 길이를 맞추기 위해 문장 끝에 eos_token를 사용\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# 패딩 토큰을 시퀀스의 어느 쪽에 추가할지 설정\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac26d2",
   "metadata": {},
   "source": [
    "- PEFT 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c18b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\t# LoRA의 스케일링 계수를 설정 한다. 값이 클 수록 학습 속도가 빨라질 수 있지만, 너무 크게 되면 모델이 불안정해질 수 있다.\n",
    "    lora_dropout=0.1,\t#  과적합을 방지하기 위한 드롭아웃 확률을 설정한다. 여기서는 10%(0.1)의 드롭아웃 확률을 사용하여 모델의 일반화 성능을 향상시킨다.\n",
    "    r=64,\t# LoRA 어댑터 행렬의 Rank를 나타낸다. 랭크가 높을수록 모델의 표현 능력은 향상되지만, 메모리 사용량과 학습 시간이 증가한다. 일반적으로 4, 8, 16, 32, 64 등의 값을 사용한다.\n",
    "    bias=\"none\",\t# LoRA 어댑터 행렬에 대한 편향을 추가할지 여부를 결정한다. “none”옵션을 사용하여 편향을 사용하지 않는다.\n",
    "    task_type=\"CAUSAL_LM\",\t# LoRA가 적용될 작업 유형을 설정한다. CAUSAL_LM은 Causal Language Modeling 작업을 의미한다. 이는 특히 GPT 같은 텍스트 생성 모델에 주로 사용된다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebff555",
   "metadata": {},
   "source": [
    "- 학습 모델 설정값 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71750fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\t# 기본값은 3\n",
    "    per_device_train_batch_size=4,\t# 기본값은 8\n",
    "    gradient_accumulation_steps=1,\t# 기본값 1\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d13450",
   "metadata": {},
   "source": [
    "- 만약 CUDA out of memory 발생할 경우 다음과 같이 batch size 및 파라미터를 수정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64944b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params = TrainingArguments(\n",
    "#     output_dir=\"/results\",\n",
    "#     num_train_epochs = 1, #epoch는 1로 설정 \n",
    "#     max_steps=5000, #max_steps을 5000으로 설정\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     warmup_steps=0.03,\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     logging_steps=100,\n",
    "#     push_to_hub=False,\n",
    "#     report_to='none',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aead660",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63106401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trl 라이브러리의 SFTTrainer클래스의 인스턴스인 trainer 객체를 사용하여 모델 학습을 시작\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\t# 256, 512 등으로 수정할 수 있음.\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직전에 정의했던 TrainingArguments와 함께 설정된 모든 매개변수를 사용하여 모델을 학습시킨다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee376c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096e0fb",
   "metadata": {},
   "source": [
    "- 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11541f90",
   "metadata": {},
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"알바생이 3일 일하고 그만뒀는데 주휴수당을 줘야 하나요?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
