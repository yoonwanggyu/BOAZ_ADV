import pymupdf4llm
import pathlib

pdf_path1 = r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818016300204-main.pdf"
pdf_path2 = r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818022000101-main.pdf"

# metadata list & example data 
def show_metadata(docs):
    if docs:
        print("[metadata]")
        print(list(docs[0].metadata.keys()))
        print("-----------------")
        print("\n[examples]")
        max_key_length = max(len(k) for k in docs[0].metadata.keys())
        for k, v in docs[0].metadata.items():
            print(f"{k:<{max_key_length}} : {v}")

# Markdown ë³€í™˜
llama_reader = pymupdf4llm.LlamaMarkdownReader()

# í˜ì´ì§€ë³„ë¡œ ì €ì¥ëœ Markdown ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸
llama_docs1 = llama_reader.load_data(pdf_path1)
llama_docs2 = llama_reader.load_data(pdf_path2)

show_metadata(llama_docs1)
print("____________________________________________________")
show_metadata(llama_docs2)

# ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
doc_list = []
for doc in llama_docs2:
    doc_list.append(doc.text)
print(doc_list[2])




# ë¶ˆí•„ìš”í•œ metadata ì •ë³´ ì œê±°
# OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬(ê³µë°±, íŠ¹ìˆ˜ ë¬¸ì ë“±)
# ëª©ì°¨ ê°ì§€ ë° êµ¬ì¡°í™”("## Introduction" ê°™ì€ ë¶€ë¶„ ì •ë¦¬)
# í˜ì´ì§€ë³„ ë§ˆí¬ë‹¤ìš´ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
# LlamaIndexì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì²­í‚¹ëœ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì €ì¥

# PDF íŒŒì¼ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (pymupdf4llm)
# í˜ì´ì§€ ë³„ ë°ì´í„° ì²­í‚¹
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ â†’ ë¶ˆí•„ìš”í•œ ê³µë°±, DOI, URL, ì¸ìš© ì‚­ì œ
# OCRì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ (pytesseract)
# ì´ë¯¸ì§€ ì† "Figure 2" ê°™ì€ í•­ëª©ì´ ë³¸ë¬¸ ì–´ë””ì—ì„œ ì„¤ëª…ë˜ëŠ”ì§€ ë§¤ì¹­
# ê²°ê³¼ë¬¼ì„ Markdown ë° JSON íŒŒì¼ë¡œ ì €ì¥


import pymupdf4llm
import re
import os
import json
from pathlib import Path
import pytesseract
from PIL import Image

# 1. PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_files = [
    r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818016300204-main.pdf",
    r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818022000101-main.pdf"
]

# 2. ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = Path("markdown_output") # ë³€í™˜ëœ Markdown íŒŒì¼ ì €ì¥
image_dir = Path("extracted_images") # PDFì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ ì €ì¥
output_dir.mkdir(exist_ok=True)
image_dir.mkdir(exist_ok=True)

# 3. PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
llama_reader = pymupdf4llm.LlamaMarkdownReader()

# 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
def preprocess_text(text):
    """ Markdown ë°ì´í„° ì •ì œ """
    text = re.sub(r"\n{3,}", "\n\n", text) 
    text = re.sub(r"\s{2,}", " ", text) # ë¶ˆí•„ìš”í•œ ê³µë°± â†’ OCR í…ìŠ¤íŠ¸ì—ì„œ ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
    text = re.sub(r"https?://\S+|doi:\S+", "", text) # í•™ìˆ ì§€ ë§í¬ ë° DOI ì •ë³´ ì‚­ì œ
    text = re.sub(r"\[\d+\]", "", text) # ì¸ìš© ê°ì£¼ ì‚­ì œ
    return text.strip()

# 5. OCRë¡œ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_image(image_path):
    """ OCRì„ ì´ìš©í•´ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ """
    img = Image.open(image_path)
    text = pytesseract.image_to_string(img, lang="eng")  # ì˜ì–´ OCR ì ìš©
    return text.strip()

# 6. íŠ¹ì • ì´ë¯¸ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë¬¸ë‹¨ ì°¾ê¸°
# "Figure 2"ë‚˜ "Fig. 2"ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ë‹¨ì„ ì°¾ì•„ì„œ í•´ë‹¹ ì´ë¯¸ì§€ì™€ ì—°ê²°
def find_related_text(page_text, figure_number):
    """
    "Figure 2", "Fig. 2" ê°™ì€ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì°¾ì•„ ê´€ë ¨ ë¬¸ë‹¨ì„ ë°˜í™˜
    """
    figure_patterns = [
        rf"Figure\s*{figure_number}",  # Figure 2
        rf"Fig\.\s*{figure_number}",  # Fig. 2
    ]

    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    paragraphs = page_text.split("\n\n")

    for para in paragraphs:
        if any(re.search(pattern, para, re.IGNORECASE) for pattern in figure_patterns):
            return para.strip()  # í•´ë‹¹ ë¬¸ë‹¨ì„ ë°˜í™˜

    return "ê´€ë ¨ ë¬¸ë‹¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"  # ë§Œì•½ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ê°’ ë°˜í™˜

# 7. PDF ë³€í™˜ & ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
def process_pdf(pdf_path):
    """ PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜í•˜ê³  ì´ë¯¸ì§€ & ê´€ë ¨ í…ìŠ¤íŠ¸ ì €ì¥ """
    pdf_name = Path(pdf_path).stem  
    metadata_list = []  

    print(f"ğŸ“– ë³€í™˜ ì¤‘: {pdf_name} ...")
    llama_docs = llama_reader.load_data(pdf_path, write_images=True, image_path=str(image_dir))

    # ì „ì²´ Markdown ì €ì¥
    full_markdown = "\n\n".join([doc.text for doc in llama_docs])
    full_markdown = preprocess_text(full_markdown)
    with open(output_dir / f"{pdf_name}.md", "w", encoding="utf-8") as f:
        f.write(full_markdown)
    print(f"ì „ì²´ Markdown ì €ì¥ ì™„ë£Œ: {pdf_name}.md")

    # í˜ì´ì§€ë³„ Markdown ì €ì¥ + ì´ë¯¸ì§€ & ê´€ë ¨ í…ìŠ¤íŠ¸ ë§¤ì¹­
    for i, doc in enumerate(llama_docs):
        page_markdown = preprocess_text(doc.text)
        page_num = i + 1  

        # ê°œë³„ í˜ì´ì§€ Markdown ì €ì¥
        with open(output_dir / f"{pdf_name}_page_{page_num}.md", "w", encoding="utf-8") as f:
            f.write(page_markdown)

        # í•´ë‹¹ í˜ì´ì§€ì—ì„œ ì €ì¥ëœ ì´ë¯¸ì§€ ì°¾ê¸°
        image_files = sorted(image_dir.glob(f"{pdf_name}-{page_num}-*.png"))
        for img_file in image_files:
            img_text = extract_text_from_image(img_file)  

            # OCRì—ì„œ "Figure 2" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
            match = re.search(r"Figure\s*(\d+)|Fig\.\s*(\d+)", img_text, re.IGNORECASE)
            figure_number = match.group(1) if match else None

            # "Figure X" íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° ê´€ë ¨ ë¬¸ë‹¨ ì°¾ê¸°
            related_text = find_related_text(page_markdown, figure_number) if figure_number else "ê´€ë ¨ ë¬¸ë‹¨ ì—†ìŒ"

            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "image_file": img_file.name,
                "pdf_name": pdf_name,
                "page_number": page_num,
                "figure_number": figure_number,
                "related_text": related_text,
                "ocr_text": img_text
            }
            metadata_list.append(metadata)

    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° JSON ì €ì¥
    with open(output_dir / f"{pdf_name}_image_metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata_list, f, indent=4, ensure_ascii=False)
    
    print(f"ğŸ“„ í˜ì´ì§€ë³„ Markdown & ì´ë¯¸ì§€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {pdf_name}")

# 8. ëª¨ë“  PDF ë³€í™˜ ì‹¤í–‰
for pdf in pdf_files:
    process_pdf(pdf)

print("ëª¨ë“  PDF ë³€í™˜ ë° ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ!")





#################################
## Recursive Character Text Split

import pymupdf
import pymupdf4llm
import re
import os
import json
from pathlib import Path
import pytesseract
from PIL import Image
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_files = [
    r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818016300204-main.pdf",
    r"C:\Users\user\OneDrive\ë°”íƒ• í™”ë©´\BOAZ\2025_ë¶„ì„_ADV session\ì±—ë´‡ í”„ë¡œì íŠ¸\GUIDELINES\1-s2.0-S0952818022000101-main.pdf"
]

# 2. ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = Path("markdown_output")  # ë³€í™˜ëœ Markdown ì €ì¥
json_dir = Path("json_output")  # JSON ì €ì¥
image_dir = Path("extracted_images")  # PDFì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ ì €ì¥
output_dir.mkdir(exist_ok=True)
json_dir.mkdir(exist_ok=True)
image_dir.mkdir(exist_ok=True)

# 3. PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê°ì²´ ìƒì„±
llama_reader = pymupdf4llm.LlamaMarkdownReader()

# 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    """ Markdown ë°ì´í„° ì •ì œ """
    text = re.sub(r"\n{3,}", "\n\n", text)  # ê°œí–‰ ì •ë¦¬
    text = re.sub(r"\s{2,}", " ", text)  # ê³µë°± ì •ë¦¬
    text = re.sub(r"https?://\S+|doi:\S+", "", text)  # DOI, URL ì‚­ì œ
    text = re.sub(r"\[\d+\]", "", text)  # ì¸ìš© ê°ì£¼ ì‚­ì œ
    return text.strip()

# 5. RecursiveCharacterTextSplitter ì„¤ì • (ì²­í‚¹)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # í•œ ì²­í¬ì˜ ìµœëŒ€ ê¸¸ì´ (1000ì)
    chunk_overlap=200,  # ì²­í¬ ê°„ 200ì ê²¹ì¹¨ â†’ ë¬¸ë§¥ ìœ ì§€
    length_function=len,
)

# 6. PDF ë©”íƒ€ë°ì´í„° ì €ì¥ í•¨ìˆ˜
def save_metadata(pdf_path, llama_docs):
    """PDFì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  JSONìœ¼ë¡œ ì €ì¥"""
    pdf_name = Path(pdf_path).stem  
    metadata_list = []

    for i, doc in enumerate(llama_docs):
        metadata = {
            "pdf_name": pdf_name,
            "page_number": i + 1,
            "title": doc.metadata.get("title", "Unknown"),
            "author": doc.metadata.get("author", "Unknown"),
            "subject": doc.metadata.get("subject", "Unknown"),
            "keywords": doc.metadata.get("keywords", "Unknown"),
            "modDate": doc.metadata.get("modDate", "Unknown"),
            "total_pages": doc.metadata.get("total_pages", "Unknown"),
            "file_path": pdf_path,
        }
        metadata_list.append(metadata)

    # JSON íŒŒì¼ë¡œ ì €ì¥
    metadata_path = json_dir / f"{pdf_name}_metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata_list, f, indent=4, ensure_ascii=False)

    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")

# 7. OCRë¡œ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_from_image(image_path):
    """ OCRì„ ì´ìš©í•´ ì´ë¯¸ì§€ ì† í…ìŠ¤íŠ¸ ì¶”ì¶œ """
    img = Image.open(image_path)
    text = pytesseract.image_to_string(img, lang="eng")  # ì˜ì–´ OCR ì ìš©
    return text.strip()

# 8. PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜ (PyMuPDF)
def extract_images_from_pdf(pdf_path, output_dir):
    """ PDFì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥ """
    pdf_document = pymupdf.open(pdf_path)
    image_count = 0
    
    for page_num in range(pdf_document.page_count):
        page = pdf_document.load_page(page_num)  # í˜ì´ì§€ ë¡œë“œ
        img_list = page.get_images(full=True)  # ì´ë¯¸ì§€ ëª©ë¡ ì¶”ì¶œ

        for img_index, img in enumerate(img_list):
            xref = img[0]
            image = pdf_document.extract_image(xref)
            image_bytes = image["image"]

            # ì´ë¯¸ì§€ ì €ì¥
            image_filename = output_dir / f"{Path(pdf_path).stem}_page_{page_num + 1}_img_{img_index + 1}.png"
            with open(image_filename, "wb") as img_file:
                img_file.write(image_bytes)
                image_count += 1

    return image_count

# 9. í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_table_to_markdown(text):
    """í…ìŠ¤íŠ¸ì—ì„œ í‘œ í˜•ì‹ ì°¾ê³  ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
    lines = text.splitlines()
    table = []
    
    for line in lines:
        if "|" in line:  # í‘œì˜ ì¤„ì„ ê°ì§€
            table.append(line.strip())
    
    if table:
        # í‘œ ë¬¸ë²•ì— ë§ê²Œ íŒŒì´í”„ ë¬¸ìë¡œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ìƒì„±
        markdown_table = "\n".join(table)
        return markdown_table
    else:
        return text  # í‘œê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜

# 10. PDF ë³€í™˜ & ì²­í‚¹ & ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_pdf(pdf_path):
    """ PDFë¥¼ Markdownìœ¼ë¡œ ë³€í™˜í•˜ê³  RecursiveCharacterTextSplitter ì ìš© """
    pdf_name = Path(pdf_path).stem  
    print(f"ë³€í™˜ ì¤‘: {pdf_name} ...")
    
    # ì´ë¯¸ì§€ ì¶”ì¶œ
    image_count = extract_images_from_pdf(pdf_path, image_dir)
    print(f"ì¶”ì¶œëœ ì´ë¯¸ì§€ ìˆ˜: {image_count}")

    # PDFì—ì„œ í…ìŠ¤íŠ¸ ë³€í™˜
    llama_docs = llama_reader.load_data(pdf_path, write_images=True, image_path=str(image_dir))
    full_text = "\n\n".join([doc.text for doc in llama_docs])
    clean_text = preprocess_text(full_text)
    
    # í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜
    clean_text_with_tables = convert_table_to_markdown(clean_text)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    save_metadata(pdf_path, llama_docs)

    # RecursiveCharacterTextSplitter ì ìš© ì²­í‚¹
    chunks = text_splitter.split_text(clean_text_with_tables)

    # ì „ì²´ Markdown ì €ì¥
    with open(output_dir / f"{pdf_name}.md", "w", encoding="utf-8") as f:
        f.write(clean_text_with_tables)
    
    print(f"ì „ì²´ Markdown ì €ì¥ ì™„ë£Œ: {pdf_name}.md")

    # ì²­í¬ë³„ JSON ì €ì¥
    chunk_data = []
    for i, chunk in enumerate(chunks):
        chunk_info = {
            "chunk_id": i + 1,
            "pdf_name": pdf_name,
            "text": chunk
        }
        chunk_data.append(chunk_info)

    with open(json_dir / f"{pdf_name}_chunks.json", "w", encoding="utf-8") as f:
        json.dump(chunk_data, f, indent=4, ensure_ascii=False)

    print(f"ì²­í¬ ë°ì´í„° JSON ì €ì¥ ì™„ë£Œ: {pdf_name}_chunks.json")

# 11. ëª¨ë“  PDF ë³€í™˜ ì‹¤í–‰
for pdf in pdf_files:
    process_pdf(pdf)

print("ëª¨ë“  PDF ë³€í™˜ ë° ì²­í‚¹ ì™„ë£Œ!")

