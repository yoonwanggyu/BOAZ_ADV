{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b67662",
   "metadata": {},
   "source": [
    "## LLaMa3.2_Prompt_converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ca8166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련 문서 최대 길이: 9개\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast  # 문자열로 저장된 리스트를 리스트 객체로 변환\n",
    "\n",
    "# 파일 경로\n",
    "file_path = r\"C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_all_final.csv\"\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# \"관련 문서\" 열의 리스트 길이 구하기\n",
    "def get_list_length(cell):\n",
    "    try:\n",
    "        # 만약 NaN이면 0 리턴\n",
    "        if pd.isna(cell):\n",
    "            return 0\n",
    "        # 문자열로 저장된 리스트를 파싱해서 길이 반환\n",
    "        parsed = ast.literal_eval(cell)\n",
    "        return len(parsed) if isinstance(parsed, list) else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# 각 행에 적용해서 길이 컬럼 추가\n",
    "df[\"관련문서_길이\"] = df[\"관련 문서\"].apply(get_list_length)\n",
    "\n",
    "# 최대 길이 찾기\n",
    "max_length = df[\"관련문서_길이\"].max()\n",
    "\n",
    "print(f\"관련 문서 최대 길이: {max_length}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "222d6bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관련 문서 최대 길이: 9개\n",
      "\n",
      "질문: 매플슨 A 호흡회로(Magill circuit)에서 재호흡을 방지하기 위해 필요한 신선가스 유량은 어떻게 계산하며, 10kg 소아에서 권장되는 신선가스 유량은 얼마인가요?\n",
      "\n",
      "관련 문서 개수: 9\n",
      "  (1) 신선가스 유량 (L/min) = 175 x BW x 0.8 (Table 1)\n",
      "  (2) Table 1. Guideline for fresh gas flow\n",
      "  (3) | Body weight (kg) | Fresh gas flow (L/min) |\n",
      "  (4) | --- | --- |\n",
      "  (5) | 5 | 0.6 |\n",
      "  (6) | 10 | 1.1 |\n",
      "  (7) | 20 | 1.9 |\n",
      "  (8) | 40 | 3.3 |\n",
      "  (9) 위의 수치는 단순한 지침이므로 재호흡이 있는 경우에는 적절하게 유량을 조절하여야 한다.\n",
      "\n",
      "답변: ##Reason: ##begin_quote## 신선가스 유량 (L/min) = 175 x BW x 0.8 (Table 1) ##end_quote## 라는 공식과 ##begin_quote## Table 1. Guideline for fresh gas flow ... | 10 | 1.1 | ##end_quote## 표에 따르면, 10kg 소아에서 권장되는 신선가스 유량은 1.1L/min입니다.\n",
      "<ANSWER>: 매플슨 A 회로에서 10kg 소아의 권장 신선가스 유량은 1.1L/min입니다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# 파일 경로\n",
    "file_path = r\"C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_all_final.csv\"\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def get_list_length(cell):\n",
    "    try:\n",
    "        if pd.isna(cell) or str(cell).strip() in ['', \"''\"]:  # 빈값 처리\n",
    "            return 0\n",
    "        parsed = ast.literal_eval(cell)\n",
    "        # 리스트 형태이며, 내용이 ''이면 0으로 취급\n",
    "        if isinstance(parsed, list):\n",
    "            if all(str(x).strip() == '' for x in parsed):\n",
    "                return 0\n",
    "            return len(parsed)\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def parse_list(cell):\n",
    "    try:\n",
    "        if pd.isna(cell) or str(cell).strip() in ['', \"''\"]:\n",
    "            return []\n",
    "        parsed = ast.literal_eval(cell)\n",
    "        if isinstance(parsed, list):\n",
    "            # 빈 문자열만 포함한 리스트면 빈 리스트로 처리\n",
    "            if all(str(x).strip() == '' for x in parsed):\n",
    "                return []\n",
    "            return parsed\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# 리스트 길이 컬럼 추가\n",
    "df[\"관련문서_길이\"] = df[\"관련 문서\"].apply(get_list_length)\n",
    "\n",
    "# 최대 길이 확인\n",
    "max_length = df[\"관련문서_길이\"].max()\n",
    "print(f\"관련 문서 최대 길이: {max_length}개\\n\")\n",
    "\n",
    "# 최대 길이 행 필터링\n",
    "longest_rows = df[df[\"관련문서_길이\"] == max_length]\n",
    "\n",
    "# 내용 출력\n",
    "for idx, row in longest_rows.iterrows():\n",
    "    print(f\"질문: {row['질문']}\\n\")\n",
    "    print(f\"관련 문서 개수: {row['관련문서_길이']}\")\n",
    "    문서목록 = parse_list(row['관련 문서'])\n",
    "    for i, 문장 in enumerate(문서목록, start=1):\n",
    "        print(f\"  ({i}) {문장}\")\n",
    "    print(f\"\\n답변: {row['답변']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae873e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf29652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) 경로 설정\n",
    "SRC_CSV   = r\"C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_train_data_result(혜원).csv\"\n",
    "DST_JSONL = r\"C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_LLaMa3.2_Fine-tuning_Dataset.jsonl\"\n",
    "\n",
    "# 2) CSV 인코딩 자동 탐색\n",
    "def read_csv_flexible(path, encodings=(\"utf-8-sig\", \"cp949\", \"euc-kr\", \"utf-8\")):\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    # 전부 안될 시 -> 최종에는 바이너리로 읽어 오류 문자 대체\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    text = raw.decode(encodings[-1], errors=\"replace\")\n",
    "    return pd.read_csv(io.StringIO(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b11ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:01<00:00, 620.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLaMA-3 JSONL dataset to C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_LLaMa3.2_Fine-tuning_Dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) 문자열로 된 리스트 파싱 (JSON 또는 Python literal)\n",
    "def safe_list_load(s: str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "# 4) LLaMA-3 Instruction 포맷 생성\n",
    "def build_llama3_instruction(question: str, ctx_list, cot_and_answer: str):\n",
    "    # 질문 앞에 'Q. ' 접두\n",
    "    q_text = f\"Q. {question.strip()}\"\n",
    "    # 문맥 Bullet들(앞에 • 붙여서 개별 문맥 표시)\n",
    "    bullets = \"\\n\".join(f\"• {c.strip()}\" for c in ctx_list if c)\n",
    "    # 조립\n",
    "    return (\n",
    "        \"<s>[INST] \"\n",
    "        f\"{q_text}\\n\\nContext:\\n{bullets}\"\n",
    "        \" [/INST] \"\n",
    "        f\"{cot_and_answer.strip()}\"\n",
    "        \" </s>\"\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    # 5) CSV 로드\n",
    "    df = read_csv_flexible(SRC_CSV)\n",
    "\n",
    "    # 6) 컬럼 정리: strip + BOM 제거\n",
    "    df.columns = [c.strip().lstrip(\"\\ufeff\") for c in df.columns]\n",
    "\n",
    "    # 7) 한글→영문 매핑 (필요하다면)\n",
    "    mapping = {\n",
    "        \"파일명\":   \"file\",\n",
    "        \"질문\":     \"question\",\n",
    "        \"관련 문서\": \"context\",\n",
    "        \"답변\":     \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in mapping.items() if k in df.columns})\n",
    "\n",
    "    # 8) 필수 컬럼 체크\n",
    "    for col in (\"question\", \"context\", \"answer\"):\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV에 '{col}' 컬럼이 없습니다.\")\n",
    "\n",
    "    # 9) JSONL으로 변환\n",
    "    os.makedirs(os.path.dirname(DST_JSONL), exist_ok=True)\n",
    "    with open(DST_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            q   = str(row[\"question\"])\n",
    "            ctx = safe_list_load(str(row[\"context\"]))\n",
    "            cot = str(row[\"answer\"])  # 이미 CoT 형식으로 ##Reason…<ANSWER>:… 를 포함\n",
    "\n",
    "            llama_text = build_llama3_instruction(q, ctx, cot)\n",
    "            fout.write(json.dumps({\"text\": llama_text}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved LLaMA-3 JSONL dataset to {DST_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1e399",
   "metadata": {},
   "source": [
    "## LLaMa3.2_Fine-tuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3cb9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da991c62d57b400e813a8e07316d9b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import huggingface_hub\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa086985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# 0) 환경 변수 & HF 로그인 (허깅페이스 API TOKEN 발급) \n",
    "load_dotenv()\n",
    "huggingface_hub.login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# 1) 설정 \n",
    "base_model     = \"meta-llama/Llama-3.2-1B\" # 베이스 모델\n",
    "dataset_path   = r\"C:\\Users\\user\\OneDrive\\바탕 화면\\BOAZ\\2025_분석_ADV session\\챗봇 프로젝트\\QAData\\학술지_LLaMa3.2_Fine-tuning_Dataset.jsonl\" # Custom Dataset\n",
    "new_model_name = \"llama3.2-1b-boaz-medical\" # 출력 모델 이름 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53c263e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5c32c4938b48828f86ba610c762139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2) JSONL 로드(우리는 전체를 train set으로 쓰고, shuffle해서 데이터 샘플 간 순서 섞기)\n",
    "ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": dataset_path},\n",
    "    split=\"train\"\n",
    ").shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627f2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) QLoRA용 4-bit 양자화 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,               # 4bit로 모델 가중치를 로드하여 메모리 사용량 크게 절감\n",
    "    bnb_4bit_quant_type=\"nf4\",       # NF4(NormalFloat4) 양자화 방식 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 내부 계산 시 float16 포맷 사용\n",
    "    bnb_4bit_use_double_quant=False, # 이중 양자화(double quantization)는 미적용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16833182",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4) 모델 로드\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 사전학습된 LLaMA-3.2 1B 모델 식별자\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# 모델 저장소의 custom code(예: Llama-3 사용자 정의)를 허용\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# 위에서 생성한 4bit 양자화 설정 적용\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# 사용 가능한 GPU/CPU 장치에 자동으로 분산 배치\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# LoRA fine-tuning 시 반드시 끄기\u001b[39;00m\n\u001b[32m     10\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m       \u001b[38;5;66;03m# generation시 사용하는 past_key/value 캐시 비활성화\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:526\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    524\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1124\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   1119\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1120\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1121\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1122\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1123\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1126\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING.keys(), key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\configuration_utils.py:764\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    762\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    767\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\models\\llama\\configuration_llama.py:160\u001b[39m, in \u001b[36mLlamaConfig.__init__\u001b[39m\u001b[34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, **kwargs)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.rope_theta = rope_theta\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.rope_scaling = rope_scaling\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rope_scaling_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_bias = attention_bias\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_dropout = attention_dropout\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\models\\llama\\configuration_llama.py:180\u001b[39m, in \u001b[36mLlamaConfig._rope_scaling_validation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.rope_scaling, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.rope_scaling) != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.rope_scaling\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    184\u001b[39m rope_scaling_type = \u001b[38;5;28mself\u001b[39m.rope_scaling.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    185\u001b[39m rope_scaling_factor = \u001b[38;5;28mself\u001b[39m.rope_scaling.get(\u001b[33m\"\u001b[39m\u001b[33mfactor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: `rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}"
     ]
    }
   ],
   "source": [
    "# 4) 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,                      # 사전학습된 LLaMA-3.2 1B 모델 식별자\n",
    "    trust_remote_code=True,          # 모델 저장소의 custom code(예: Llama-3 사용자 정의)를 허용\n",
    "    quantization_config=quant_config,# 위에서 생성한 4bit 양자화 설정 적용\n",
    "    device_map=\"auto\",               # 사용 가능한 GPU/CPU 장치에 자동으로 분산 배치\n",
    ")\n",
    "\n",
    "# LoRA fine-tuning 시 반드시 끄기\n",
    "model.config.use_cache = False       # generation시 사용하는 past_key/value 캐시 비활성화\n",
    "model.config.pretraining_tp = 1      # tensor parallelism을 1로 고정(단일 GPU 모드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,                      # 동일한 base_model에서 토크나이저 불러오기\n",
    "    trust_remote_code=True,          # custom tokenizer 코드 허용\n",
    ")\n",
    "\n",
    "tokenizer.pad_token    = tokenizer.eos_token  # 패딩 토큰을 eos_token과 동일하게 설정\n",
    "tokenizer.padding_side = \"right\"              # 오른쪽에 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) LoRA(PEFT) 설정\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",   # LoRA 적용 - 답변 생성 Task\n",
    "    inference_mode=False,    # 학습 모(LoRA 가중치 학습 허용)\n",
    "    r=32,                    # LoRA low-rank 차원(rank) -> 작을수록 메모리·연산량 감소\n",
    "    lora_alpha=16,           # LoRA 스케일링 계수(alpha) -> 작을수록 보수적으로 학습\n",
    "    lora_dropout=0.1,        # 10% 드롭아웃으로 과적합 방지\n",
    "    bias=\"none\",             # LoRA 어댑터에는 편향(bias) 레이어 비활성화(편향까지 추가하면 불안정해질 수 있음)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ddd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) 학습 인자\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_llama3_1b\",     # 체크포인트와 로그 저장 디렉터리\n",
    "    per_device_train_batch_size=1,        # GPU 당 배치 사이즈, 실제 GPU 하나당 매 스텝에 1개 샘플을 로드\n",
    "    gradient_accumulation_steps=8,        # 매 8 스텝마다 한 번씩 역전파를 수행해 효과적으로 배치 크기를 8로 늘린 것과 같음\n",
    "    num_train_epochs=3,                   # 전체 데이터를 3회 반복 학습\n",
    "    learning_rate=2e-4,                   # 학습률 0.0002\n",
    "    optim=\"paged_adamw_32bit\",            # 32-bit AdamW 옵티마이저 (사용 빈도가 낮은 파라미터 페이지(메모리 블록)를 자동으로 CPU 메모리나 디스크로 옮겼다가, 필요할 때만 GPU로 불러오는 방식)\n",
    "    fp16=True,                            # FP16 mixed-precision 활성화\n",
    "    bf16=False,                           # BF16 사용 안 함\n",
    "    save_steps=50,                        # 50 스텝마다 모델 저장\n",
    "    logging_steps=50,                     # 50 스텝마다 로깅\n",
    "    weight_decay=0.01,                    # 가중치 감쇠 0.01, L2계수\n",
    "    warmup_ratio=0.03,                    # 전체의 3%만큼 워밍업 단계, 워밍업 단계에 learning rate를 선형 증가시켜 안정적인 학습을 도움\n",
    "    max_grad_norm=0.3,                    # 그래디언트 클리핑 최대 L2 노름 0.3\n",
    "    lr_scheduler_type=\"constant\",         # constant : 학습 내내 고정된 학습률을 사용\n",
    "    report_to=\"tensorboard\",              # 텐서보드로 로그 전송\n",
    "    group_by_length=True,                 # 길이가 비슷한 시퀀스끼리 묶어서 배치로 구성, 패딩 최소화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) SFTTrainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                       # fine-tune할 모델\n",
    "    train_dataset=ds,                  # 학습 데이터셋\n",
    "    peft_config=peft_config,           # LoRA 설정\n",
    "    dataset_text_field=\"text\",         # 데이터셋 텍스트 필드 이름\n",
    "    tokenizer=tokenizer,               # 토크나이저\n",
    "    args=training_args,                # 학습 인자\n",
    "    packing=False,                     # packing 비활성화(토크나이저 별도, 문장별 경계를 명확히 유지해야 할 때 주로 사용)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) 학습 & 저장\n",
    "trainer.train()                       # Fine-tuning 시작\n",
    "trainer.save_model(new_model_name)    # 학습된 LoRA 어댑터 + Config 저장\n",
    "print(\"✅ Fine-tuning 완료. Saved to:\", new_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
