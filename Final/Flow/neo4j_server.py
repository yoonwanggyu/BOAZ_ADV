# from mcp.server.fastmcp import FastMCP
# from dotenv import load_dotenv
# from langchain_community.graphs import Neo4jGraph
# from langchain_neo4j import Neo4jGraph
# import os
# from openai import OpenAI
# from typing import List, Dict

# load_dotenv("/Users/yoon/BOAZ_ADV/Wang_Gyu/code/mcp/.env")

# # Neo4j ì—°ê²°
# neo4j_graph = Neo4jGraph(
#     url = os.getenv("NEO4J_URI"),  
#     username=os.getenv("NEO4J_USER"),
#     password=os.getenv("NEO4J_PASSWORD"),  
#     refresh_schema=False)

# # Tool description ìì„¸íˆ ì“°ê¸°
# mcp = FastMCP(
#     "Neo4j_Retriever",
#     instructions="A Retriever that can retrieve information from the Neo4j database.",
#     host="0.0.0.0",
#     port=8005,
# )

# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# def _embed(text: str) -> List[float]:
#     """OpenAI text-embedding-3-large â†’ 3072-dim list[float]."""
#     resp = client.embeddings.create(
#         model="text-embedding-3-large",
#         input=text.replace("\n", " ")[:8192]  # í† í° í•œë„ ëŒ€ë¹„
#     )
#     return resp.data[0].embedding

# @mcp.tool()
# def neo4j_retriever(question: str, top_k: int = 3) -> List[Dict]:
#     """
#     ìì—°ì–´ ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ì—¬ (:Patient.embedding) ë²¡í„° ì¸ë±ìŠ¤ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ Top-k í™˜ìë¥¼ ë°˜í™˜.
#     """
#     query_vec = _embed(question)

#     cypher = """
#     CALL db.vector.similarity.matchNodes(
#         'patient_embedding_vector_idx',  // ì¸ë±ìŠ¤ ì´ë¦„
#         $vec,                            // ì¿¼ë¦¬ ë²¡í„°
#         $k                               // Top-k
#     ) YIELD node, score
#     RETURN node.name AS name, score
#     """

#     rows = neo4j_graph.query(cypher, {"vec": query_vec, "k": top_k})
#     return [{"name": r["name"], "score": r["score"]} for r in rows]

# if __name__ == "__main__":
#     mcp.run(transport="stdio")


from neo4j import GraphDatabase
from neo4j_graphrag.llm import OpenAILLM
from neo4j_graphrag.retrievers import VectorRetriever
from neo4j_graphrag.embeddings.openai import OpenAIEmbeddings
import os
from mcp.server.fastmcp import FastMCP
from embedder import *
from dotenv import load_dotenv

load_dotenv("/Users/yoon/BOAZ_ADV/Wang_Gyu/code/mcp/.env")

embedder = SMCEmbeddings(model="text-embedding-3-large", 
                         dimensions=256, 
                         api_key=os.getenv("OPENAI_API_KEY"))

NEO4J_URI = "neo4j+s://fbc3f4e2.databases.neo4j.io"
AUTH = ("neo4j", "VoHB3kKRDdUoX7jvbGnTr4toga6-MNlvFyN3HIIsoyE")
DATABASE = "neo4j"

# ---- Neo4j ì—°ê²° ----
driver = GraphDatabase.driver(NEO4J_URI, auth=AUTH)

# ---- LLM ì„¤ì • (ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©) ----
llm = OpenAILLM(model_name="gpt-4", model_params={"temperature": 0})

# ---- ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ê¸° ì„¤ì • ----
retriever = VectorRetriever(
    driver,
    index_name="entity_vector",
    embedder=embedder,
    return_properties=["name"],
)

mcp = FastMCP(
    "Neo4j_Retriever",
    instructions="A Retriever that can retrieve information from the Neo4j database.",
    host="0.0.0.0",
    port=8005,
)

# ---- elementIdë¡œ 1-hop ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ----
def get_node_with_neighbors(driver, element_id):
    query = """
    MATCH (n) WHERE elementId(n) = $elementId
    OPTIONAL MATCH (n)-[r]-(m)
    RETURN 
      n {.name, labels: labels(n)} AS starting_node,
      collect({
        neighbor_name: m.name,
        text: m.text,
        neighbor_labels: labels(m),
        relation: type(r),
        direction: CASE WHEN startNode(r) = n THEN 'out' ELSE 'in' END
      }) AS neighbors
    """
    with driver.session() as session:
        result = session.run(query, elementId=element_id)
        return result.single()

# ---- context ìƒì„± í•¨ìˆ˜ ----
import time

def build_context_from_vector(driver, query_text: str, top_k: int = 5) -> str:
    print(f"\nğŸ” [Context Builder] ì‹œì‘ - Query: '{query_text}', Top-k: {top_k}")
    total_start = time.time()
    
    # 1. query_textë¥¼ embeddingìœ¼ë¡œ ë³€í™˜
    print("ğŸ“Š Step 1: Embedding ìƒì„± ì¤‘...")
    embed_start = time.time()
    query_vector = embedder.embed_query(query_text, dimensions=256)
    embed_time = time.time() - embed_start
    print(f"   âœ… Embedding ì™„ë£Œ - ì‹œê°„: {embed_time:.2f}ì´ˆ")

    # 2. search ì‹œ query_textëŠ” ì œê±°í•˜ê³  query_vectorë§Œ ì „ë‹¬
    print("ğŸ” Step 2: Vector ê²€ìƒ‰ ì¤‘...")
    search_start = time.time()
    retriever_result = retriever.search(query_vector=query_vector, top_k=top_k)
    search_time = time.time() - search_start
    print(f"   âœ… Vector ê²€ìƒ‰ ì™„ë£Œ - ì‹œê°„: {search_time:.2f}ì´ˆ")
    print(f"   ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(retriever_result.items)}ê°œ")
    
    # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
    for i, item in enumerate(retriever_result.items):
        score = getattr(item, 'score', 'N/A')
        element_id = item.metadata.get('id', 'Unknown')
        print(f"      [{i+1}] Element ID: {element_id}, Score: {score}")

    # 3. 1-hop í™•ì¥
    print("ğŸŒ Step 3: 1-hop ë…¸ë“œ í™•ì¥ ì¤‘...")
    expand_start = time.time()
    expanded_contexts = []

    for i, item in enumerate(retriever_result.items):
        element_id = item.metadata['id']
        print(f"   ğŸ”— [{i+1}/{len(retriever_result.items)}] Element ID: {element_id} ì²˜ë¦¬ ì¤‘...")
        
        node_start = time.time()
        node_info = get_node_with_neighbors(driver, element_id)
        node_time = time.time() - node_start
        
        if node_info:
            start_node = node_info['starting_node']
            neighbors_count = len(node_info['neighbors'])
            print(f"      âœ… ë…¸ë“œ '{start_node.get('name', 'Unknown')}' ì²˜ë¦¬ ì™„ë£Œ")
            print(f"         - ì´ì›ƒ ë…¸ë“œ ìˆ˜: {neighbors_count}ê°œ")
            print(f"         - ì²˜ë¦¬ ì‹œê°„: {node_time:.2f}ì´ˆ")
            expanded_contexts.append(node_info)
        else:
            print(f"      âŒ ë…¸ë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì‹œê°„: {node_time:.2f}ì´ˆ")
    
    expand_time = time.time() - expand_start
    print(f"   âœ… 1-hop í™•ì¥ ì™„ë£Œ - ì‹œê°„: {expand_time:.2f}ì´ˆ")
    print(f"   ğŸ“Š ì„±ê³µì ìœ¼ë¡œ í™•ì¥ëœ ë…¸ë“œ ìˆ˜: {len(expanded_contexts)}ê°œ")

    # 4. Context êµ¬ì„±
    print("ğŸ“ Step 4: Context í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘...")
    context_start = time.time()
    context_lines = []
    total_neighbors = 0
    
    for ctx_idx, ctx in enumerate(expanded_contexts):
        start = ctx['starting_node']
        start_name = start.get('name', 'Unknown')
        start_labels = [l for l in start.get('labels', []) if l not in {'__KGBuilder__', '__Entity__'}]
        context_lines.append(f"starting_node: {start_name} ({', '.join(start_labels)})")
        
        neighbors_count = len(ctx['neighbors'])
        total_neighbors += neighbors_count
        print(f"   ğŸ“ [{ctx_idx+1}] '{start_name}' - ì´ì›ƒ {neighbors_count}ê°œ ì²˜ë¦¬ ì¤‘...")

        for neighbor in ctx['neighbors']:
            labels = [l for l in neighbor.get('neighbor_labels', []) if l not in {'__KGBuilder__', '__Entity__'}]
            label_str = f"[{', '.join(labels)}]" if labels else "[Unknown]"

            # Chunk ë…¸ë“œë©´ í…ìŠ¤íŠ¸, ì•„ë‹ˆë©´ name
            if 'Chunk' in neighbor.get('neighbor_labels', []):
                neighbor_value = neighbor.get('text', 'None')
                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ
                display_value = neighbor_value[:50] + "..." if len(neighbor_value) > 50 else neighbor_value
                print(f"      ğŸ§© Chunk ë…¸ë“œ: {display_value}")
            else:
                neighbor_value = neighbor.get('neighbor_name', 'None')
                print(f"      ğŸ”— Entity ë…¸ë“œ: {neighbor_value}")

            relation = neighbor.get('relation', 'UNKNOWN_RELATION')
            direction = neighbor.get('direction', 'out')

            if direction == 'out':
                triple = f"({start_name}, {relation}, {neighbor_value})"
            else:
                triple = f"({neighbor_value}, {relation}, {start_name})"

            context_lines.append(f"{label_str} : \"{neighbor_value}\" {triple}")

    return "\n".join(context_lines)

@mcp.tool()
def run_contextual_rag(query_text: str):
    """Neo4j ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # print(f"\n[ì§ˆë¬¸] {query_text}\n")

        context = build_context_from_vector(driver, query_text, top_k=2)

        # print("[ğŸ” ìƒì„±ëœ Context]\n")
        # print(context)

        prompt = f"context: {context}\n\nì§ˆë¬¸: {query_text}"
        response = llm.invoke(prompt)
        # # ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ LLM í˜¸ì¶œ
        # response = llm.invoke(context=context, question=query_text)
        
        # print("\n[ğŸ§  LLM ì‘ë‹µ]\n")
        print(response)
        
        return response
        
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(error_msg)
        return error_msg
    
    # print("\n[ğŸ§  LLM ì‘ë‹µ]\n")
    # print(response)

# run_contextual_rag("ìœ¤ì™•ê·œ í™˜ìì˜ ë‚˜ì´ì™€ ì„±ë³„, ì²˜ë°©ë°›ì€ ì•½ë¬¼ì€?")

if __name__ == "__main__":
    mcp.run(transport="stdio")
# if __name__ == "__main__":
#     mcp.run(transport="socket")