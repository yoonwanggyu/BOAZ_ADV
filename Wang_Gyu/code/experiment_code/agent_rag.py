from langchain_community.graphs import Neo4jGraph
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv
from typing import Annotated, List, TypedDict
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import AIMessage,HumanMessage
import json
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from openai import OpenAI
from langchain.chat_models import ChatOpenAI
import requests
import socket
import time

load_dotenv()

# MCP ì„¤ì •
MCP_HOST = os.getenv("MCP_HOST", "localhost")
MCP_PORT = int(os.getenv("MCP_PORT", "5000"))
SLACK_CHANNEL = os.getenv("SLACK_CHANNEL", "#medical-alerts")

# Neo4j ì—°ê²°
neo4j_graph = Neo4jGraph(
    url="bolt://localhost:7687",  
    username="neo4j",
    password="123456789",  
    refresh_schema=False)

# LLM ì •ì˜ (OpenAI ì‚¬ìš© ì˜ˆì‹œ)
llm = ChatOpenAI(model="gpt-4", 
                 temperature=0.3)

# GraphState ìƒíƒœ ì •ì˜
class ChatbotState(TypedDict):
    question: Annotated[str, "Question"] 
    neo4j_data: Annotated[List, "Context"]  
    vector_data: Annotated[List, "Context"]  
    answer: Annotated[str, "Answer"]  
    messages: Annotated[List, add_messages]  
    tools : Annotated[List,"tool"]

# ì§ˆë¬¸ì„ Cypherë¡œ ë³€í™˜ í•¨ìˆ˜ í›„ ê²€ìƒ‰ëœ ê°’ ë¦¬í„´
def change_query_to_cypher(state: ChatbotState):

    CYPHER_GENERATION_TEMPLATE = PromptTemplate.from_template("""
# Task:
Generate a Cypher query to answer the user's question using the provided graph schema.

# Guidelines:
- Use only the relationship types and node properties defined in the schema.
- Use MATCH and RETURN clauses to extract only the necessary properties.
- Always filter using WHERE clauses with node properties when appropriate (e.g., p.name CONTAINS "xxx").
- If the question expects a count or number, use count(*) with an alias.
- If descriptive fields are expected, use RETURN ... AS [Korean field name] style.
- Avoid returning full nodes unless explicitly requested.
- If the question refers to information not present in the schema, respond with: `UNSUPPORTED_QUERY`

# Schema (Node:Label and Relationship):
(:Patient)-[:UNDERWENT]->(:Surgery)
(:Surgery)-[:HAS_SYMPTOM]->(:Symptom)
(:Surgery)-[:HAS_PROCEDURE]->(:Procedure)
(:Surgery)-[:HAS_RESULT]->(:Result)
(:Surgery)-[:HAS_NOTE]->(:Note)

# Examples:
Question: How many surgeries has Patient A undergone?
MATCH (p:Patient)-[:UNDERWENT]->(s:Surgery) WHERE p.name CONTAINS  "Patient A" RETURN count(s) AS ìˆ˜ìˆ íšŸìˆ˜

Question: What is the result of í™˜ì•„ B's surgery?
MATCH (p:Patient)-[:UNDERWENT]->(s:Surgery)-[:HAS_RESULT]->(r:Result) WHERE p.name CONTAINS  "í™˜ì•„ B" RETURN s.name AS ìˆ˜ìˆ ëª…, r.description AS ìˆ˜ìˆ í›„ìƒíƒœ
# User's Question:
{question}
""")
    
    query = state["question"]
    
    CYPHER_GENERATION_PROMPT = CYPHER_GENERATION_TEMPLATE.format(question=query)

    cypher_query = llm.invoke(CYPHER_GENERATION_PROMPT)

    if isinstance(cypher_query, AIMessage):
        response_text = cypher_query.content
    else:
        response_text = str(cypher_query)

    data = neo4j_graph.query(response_text)

    return ChatbotState(neo4j_data=[json.dumps(data, ensure_ascii=False)])

# ê·¸ë˜í”„ db ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
def generate_cypher_to_answer(state: ChatbotState):

    prompt = PromptTemplate.from_template("""You are a helpful assistant for medical patient queries.
                Based on the data below from the knowledge graph, answer the user's question.

                Question: {question}

                Graph Results:{results}

                Answer:
                """)
    
    query = state["question"]
    docs = state['documents']
    
    formatted_prompt = prompt.format(question = query, results = docs)

    response = llm.invoke(formatted_prompt) 

    if isinstance(response, AIMessage):
        response_text = response.content
    else:
        response_text = str(response)

    return ChatbotState(answer=response_text,
                        messages=[HumanMessage(content=query),AIMessage(content=response_text)])


# vector dbì—ì„œ ë¬¸ì„œ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ
def retrieve_vector_db(state:ChatbotState):

    query = state["question"]
    
    edit_path = "./edit_db"
    
    load_edit_db = Chroma(persist_directory=edit_path,
                        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
                        collection_name="edit")
    
    reranker_model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
    compressor_retriever = CrossEncoderReranker(model=reranker_model, 
                                                top_n=5)
    
    edit_db_retriever = load_edit_db.as_retriever(search_kwargs={"k": 10})
    edit_retriever = ContextualCompressionRetriever(base_retriever=edit_db_retriever, 
                                                    base_compressor=compressor_retriever)
    
    docs = edit_retriever.invoke(query)

    return ChatbotState(vector_data = docs)

# ë²¡í„° dbì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
def generate_answer(state:ChatbotState):

    query = state["question"]
    docs = state['vector_data']

    prompt = PromptTemplate.from_template("""You are a helpful assistant for medical patient queries.
                Based on the data below from the knowledge graph, answer the user's question accurately and concisely.
                Only use the information retrieved â€” do not add any extra or speculative content.

                User's Question:
                {user_question}

                Retrieved Documents:
                {retrieved_content}

                Your Response:

                """)
    
    formatted_prompt = prompt.format(user_question = query, retrieved_content = docs)

    response = llm.invoke(formatted_prompt) 

    if isinstance(response, AIMessage):
        response_text = response.content
    else:
        response_text = str(response)

    return ChatbotState(answer=response_text,
                        messages=[HumanMessage(content=query),AIMessage(content=response_text)])

# ì´ë©”ì¼ ë³´ë‚´ëŠ” ë…¸ë“œ
def send_email(state: ChatbotState):
    try:
        # MCP ë©”ì‹œì§€ í¬ë§·
        mcp_message = {
            "type": "slack_message",
            "channel": SLACK_CHANNEL,
            "content": {
                "blocks": [
                    {
                        "type": "header",
                        "text": {
                            "type": "plain_text",
                            "text": "ğŸš¨ ì˜ë£Œ ì•Œë¦¼",
                            "emoji": True
                        }
                    },
                    {
                        "type": "section",
                        "fields": [
                            {
                                "type": "mrkdwn",
                                "text": f"*í™˜ì ì§ˆë¬¸:*\n{state['question']}"
                            },
                            {
                                "type": "mrkdwn",
                                "text": f"*ì²˜ë¦¬ ê²°ê³¼:*\n{state.get('answer', 'ì²˜ë¦¬ ì¤‘')}"
                            }
                        ]
                    }
                ]
            }
        }

        # MCP ì„œë²„ì— ì—°ê²°
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.connect((MCP_HOST, MCP_PORT))
            # ë©”ì‹œì§€ ì „ì†¡
            s.sendall(json.dumps(mcp_message).encode('utf-8'))
            # ì‘ë‹µ ëŒ€ê¸°
            response = s.recv(1024).decode('utf-8')
            
            if response:
                return ChatbotState(
                    answer="MCPë¥¼ í†µí•´ Slackìœ¼ë¡œ ì•Œë¦¼ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    messages=[HumanMessage(content=state["question"]), AIMessage(content="MCPë¥¼ í†µí•´ Slackìœ¼ë¡œ ì•Œë¦¼ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.")]
                )
            else:
                return ChatbotState(
                    answer="MCP ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: ì‘ë‹µ ì—†ìŒ",
                    messages=[HumanMessage(content=state["question"]), AIMessage(content="MCP ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: ì‘ë‹µ ì—†ìŒ")]
                )
            
    except ConnectionRefusedError:
        return ChatbotState(
            answer="MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            messages=[HumanMessage(content=state["question"]), AIMessage(content="MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨: ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.")]
        )
    except Exception as e:
        return ChatbotState(
            answer=f"MCP ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            messages=[HumanMessage(content=state["question"]), AIMessage(content=f"MCP ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")]
        )

# tool(í•¨ìˆ˜) ëª©ë¡ ì •ì˜
tools = [
    {
        "type": "function",
        "function": {
            "name": "change_query_to_cypher",
            "description": "í™˜ìì˜ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ê·¸ë˜í”„ DBì—ì„œ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Patient name, e.g., í™˜ì•„ A"
                    }
                },
                "required": ["name"],
                "additionalProperties": False
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "retrieve_vector_db",
            "description": "ì˜í•™ ì •ë³´ë¥¼ Chroma Vector DBì—ì„œ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Query, e.g., í˜ˆì†ŒíŒê°ì†Œì¦ì˜ ì£¼ëœ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"
                    }
                },
                "required": ["query"],
                "additionalProperties": False
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "send_email",
            "description": "ì´ë©”ì¼ì„ ë³´ë‚´ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Query, e.g., ê°„í˜¸ì‚¬1í•œí…Œ ì´ë©”ì¼ì¢€ ë³´ë‚´ì¤˜."
                    }
                },
                "required": ["query"],
                "additionalProperties": False
            }
        }
    }
]

client = OpenAI()
# ì–´ë–¤ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí• ì§€ ê²°ì •í•˜ëŠ” ì—ì´ì „íŠ¸
def agent_llm(state: ChatbotState):
    query = state["question"]
    
    messages = [
        {"role": "system", "content": "You are a helpful medical assistant that can use various tools to help answer questions."},
        {"role": "user", "content": query}
    ]

    completion = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        tools=tools,
        tool_choice="auto",               
        parallel_tool_calls=True)

    assistant_message = completion.choices[0].message
    tool_calls = assistant_message.tool_calls

    tools = []
    for tool_call in tool_calls:
        function_name = tool_call.function.name
        tools.append(function_name)
        function_args = json.loads(tool_call.function.arguments)

    return ChatbotState(tools=tools)

# ìµœì¢… ì„ íƒëœ ë…¸ë“œë“¤ì˜ ê²°ê³¼ê°’ì„ í•©ì¹˜ëŠ” LLM
def aggregate_node(state:ChatbotState):
    tool_list = state['tools']
    neo4j_data = state.get('neo4j_data', [])
    vector_data = state.get('vector_data', [])
    answer = state.get('answer', '')
    
    # Combine all available information
    combined_context = []
    if neo4j_data:
        combined_context.append("Graph Database Results:")
        combined_context.extend(neo4j_data)
    if vector_data:
        combined_context.append("\nVector Database Results:")
        combined_context.extend([str(doc) for doc in vector_data])
    
    # Generate final response
    prompt = PromptTemplate.from_template("""You are a helpful medical assistant. Please provide a comprehensive answer 
    based on all available information. If there are conflicting information, prioritize the most recent or most specific data.

    User's Question: {question}

    Available Information:
    {context}

    Please provide a clear and concise response that addresses the user's question.
    """)
    
    formatted_prompt = prompt.format(
        question=state["question"],
        context="\n".join(combined_context)
    )
    
    response = llm.invoke(formatted_prompt)
    
    if isinstance(response, AIMessage):
        final_response = response.content
    else:
        final_response = str(response)
    
    return ChatbotState(
        answer=final_response,
        messages=[HumanMessage(content=state["question"]), AIMessage(content=final_response)]
    )

# ê·¸ë˜í”„ ì •ì˜
builder = StateGraph(ChatbotState)

# ë…¸ë“œ ì •ì˜
builder.add_node("change_query_to_cypher", change_query_to_cypher)
builder.add_node("generate_cypher_to_answer", generate_cypher_to_answer)
builder.add_node("retrieve_vector_db", retrieve_vector_db)
builder.add_node("generate_answer", generate_answer)
builder.add_node("send_mail", send_email)
builder.add_node("aggregate_node", aggregate_node)
builder.add_node("agent_llm", agent_llm)

# ë…¸ë“œ ì—°ê²°
builder.add_edge(START, "agent_llm")
builder.add_edge("agent_llm", "change_query_to_cypher", condition=lambda x: "change_query_to_cypher" in x["tools"])
builder.add_edge("agent_llm", "retrieve_vector_db", condition=lambda x: "retrieve_vector_db" in x["tools"])
builder.add_edge("agent_llm", "send_mail", condition=lambda x: "send_email" in x["tools"])
builder.add_edge("change_query_to_cypher", "generate_cypher_to_answer")
builder.add_edge("retrieve_vector_db", "generate_answer")
builder.add_edge("generate_cypher_to_answer", "aggregate_node")
builder.add_edge("generate_answer", "aggregate_node")
builder.add_edge("send_mail", "aggregate_node")
builder.add_edge("aggregate_node", END)

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "í™˜ì•„ Aì˜ ìˆ˜ìˆ  ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = ChatbotState(
        question=test_query,
        neo4j_data=[],
        vector_data=[],
        answer="",
        messages=[],
        tools=[]
    )
    
    # ê·¸ë˜í”„ ì‹¤í–‰
    try:
        result = graph.invoke(initial_state)
        print("\nìµœì¢… ì‘ë‹µ:")
        print(result["answer"])
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")